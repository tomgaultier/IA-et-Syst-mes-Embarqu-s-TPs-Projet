{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "efc9100a06c04d0b8b17c557d7cbbd41",
        "deepnote_cell_type": "markdown",
        "id": "EaiUu_PbLAjO",
        "tags": []
      },
      "source": [
        "## Introduction à la quantization \n",
        "\n",
        "Laurent cetinsoy\n",
        "\n",
        "Les réseaux de neurones prennent beaucoup de place et il peut être difficile de les faire rentrer sur certains dispositifs embarqués. \n",
        "\n",
        "Il existe plusieurs méthodes pour réduire la taille et augmenter la vitesse d'executer des réseaux de neurone. Par exemple il y a ce qu'on appelle la quantization et le pruning.\n",
        "\n",
        "Dans ce notebook on va faire une introduction à la quantization avec la librairie tensorflow lite.\n",
        "\n",
        "\n",
        "## Quantization post training\n",
        "\n",
        "Dans un premier temps on va quantifier notre réseau après l'avoir entraîné normalement. \n",
        "\n",
        "\n",
        "Entraîner un réseau de neurone convolutionnel simple avec keras pour faire de la classification MNIST (ou un autre dataset simple de votre choix si (vous en avez marre de ce dataset - https://keras.io/api/datasets/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "87bc3d9f649e416983c0ee9b59bd2de6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "_buJDoJaLAjR",
        "outputId": "9d733b41-7c20-412f-a554-fcd4c395b101",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 211s 112ms/step - loss: 0.1040 - accuracy: 0.9679 - val_loss: 0.0460 - val_accuracy: 0.9849\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a84a150d0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from tensorflow.keras.datasets.mnist import load_data\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 28, 28, 1) / 255\n",
        "x_train.shape\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255\n",
        "x_train.shape\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b984ac621a2249d1b955535f9f9ab55b",
        "deepnote_cell_type": "markdown",
        "id": "GMlOroVTLAjS",
        "tags": []
      },
      "source": [
        "Afficher le nombre de paramètre du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "11935acab177492cbcd4a8aafea0bdf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "qVOxirK4LAjT",
        "outputId": "f47caf09-bb64-4903-d933-0066029412bb",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 200)               1843400   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                2010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,864,226\n",
            "Trainable params: 1,864,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4782a0f186d14d4e91cfd903998f8710",
        "deepnote_cell_type": "markdown",
        "id": "amPAKiZGLAjT",
        "tags": []
      },
      "source": [
        "Sauvegarder votre modèle et afficher la taille du fichier. Si on applique une bête règle de trois, quelle est la taille occupée par paramètre ? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "8f50426e286c4cf39610c41ed5cdfeb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "U6FTb1PALAjU",
        "outputId": "7d9dbb14-7709-41c2-db40-d02e4e99f74c",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du fichier:  22414000 octets\n",
            "Taille occupée par parametre:  12.023220360621513 octets\n"
          ]
        }
      ],
      "source": [
        "model.save(\"model.h5\")\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Taille du fichier: \",os.path.getsize(\"model.h5\"), \"octets\" )\n",
        "\n",
        "print(\"Taille occupée par parametre: \", os.path.getsize(\"model.h5\")/model.count_params() , \"octets\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "2e0f0d5c02dc40db9fae96aa67d4de06",
        "deepnote_cell_type": "markdown",
        "id": "yN3HQ5piLAjU",
        "tags": []
      },
      "source": [
        "On va maintenant convertir notre modèle keras en modèle tensorflow lite. \n",
        "\n",
        "Installer la librairie tensorflow lite créer une instance de la class TFLiteConverter à partir de votre modèle keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "8c1f6d62ff6a416aa3ec3b05ebccddb9",
        "deepnote_cell_type": "code",
        "id": "ZQpe8RYJLAjU",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"model.h5\")\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "13a2cc6cc09f4e799d4e8641521ece8e",
        "deepnote_cell_type": "markdown",
        "id": "ZjHV1i28LAjV",
        "tags": []
      },
      "source": [
        "Convertir votre modèle et le sauvegarder dans un fichier nommé model.tflite. Sa taille est-elle plus petite ? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "2ca07a9b6ee74a6594c3a33313decfc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "75GhA8gLLAjV",
        "outputId": "bfdd70ca-6eaa-4e9d-e0e3-f9d07cde038c",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du fichier Keras:  22414000 octets\n",
            "Taille du fichier Tensorflow Lite:  7460192 octets\n"
          ]
        }
      ],
      "source": [
        "model_quantized = converter.convert()\n",
        "\n",
        "with open('model_quantized.tflite', 'wb') as f:\n",
        "  f.write(model_quantized)\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Taille du fichier Keras: \",os.path.getsize(\"model.h5\"), \"octets\" )\n",
        "print(\"Taille du fichier Tensorflow Lite: \",os.path.getsize(\"model_quantized.tflite\"), \"octets\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0e707074068f44ffad07b9301948782b",
        "deepnote_cell_type": "markdown",
        "id": "gWVGSzBsLAjV",
        "tags": []
      },
      "source": [
        "On va maintenant spécifier des optimisations au converter. \n",
        "\n",
        "1. Recréer un converter\n",
        "\n",
        "2. modifier son attribut optimizations pour ajouter une liste d'optimisation avec la valeur tf.lite.Optimize.DEFAULT\n",
        "\n",
        "3. Relancer la conversion du modèle, sauvegarder le modèle et regarder la taille du fichier généré"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "42b10a79262949aeadb6f2f82eae6f58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "izFKkXi5LAjW",
        "outputId": "7ec8abde-8f5b-46ee-c7cf-e8e2ce89f311",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du modèle optimisé: 1870216 octets\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"model.h5\")\n",
        "\n",
        "converter_optimized = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "converter_optimized.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "model_quantized_optimized = converter_optimized.convert()\n",
        "\n",
        "with open('model_quantized_optimized.tflite', 'wb') as f:\n",
        "    f.write(model_quantized_optimized)\n",
        "  \n",
        "import os\n",
        "print(\"Taille du modèle optimisé:\", os.path.getsize('model_quantized_optimized.tflite'), \"octets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e083971b3c7647818e1532ed9c7edb41",
        "deepnote_cell_type": "markdown",
        "id": "52TXaAstLAjW",
        "tags": []
      },
      "source": [
        "Quelle type  de quantization Optimize.Default, utilise-t-elle ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c4952d60d77243439193b6e9b4d573e9",
        "deepnote_cell_type": "markdown",
        "id": "vq5kCEFBLAjW",
        "tags": []
      },
      "source": [
        "L'optimisation tf.lite.Optimize.DEFAULT utilise une combinaison de quantification de poids et d'activation ainsi que d'autres optimisations telles que l'élagage et la fusion de couches. Cette combinaison d'optimisations peut réduire considérablement la taille du modèle et améliorer les performances d'inférence sur les appareils embarqués.\n",
        "\n",
        "Plus précisément, lors de l'utilisation de tf.lite.Optimize.DEFAULT, le convertisseur TensorFlow Lite utilise par défaut la pondération 8 bits et active la quantification. Cela signifie que les pondérations et les activations du modèle sont converties en entiers 8 bits au lieu de flottants 32 bits, ce qui réduit considérablement la taille du modèle. Le convertisseur peut également nettoyer en supprimant les poids qui ont peu d'importance relative. Enfin, le convertisseur peut fusionner certaines couches du modèle pour réduire davantage la taille du modèle. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0bfeb7a588454ae6942059d7d79be609",
        "deepnote_cell_type": "markdown",
        "id": "Brv6BgnILAjW",
        "tags": []
      },
      "source": [
        "## Quantization aware training \n",
        "\n",
        "Dans cette section on va s'intéresser à l'entraînement sensible à la quantification. L'idée est de simuler les effets de la quantification pendant l'entraînement pour que le modèle ajuste les poids afin de tenir ocmpte de la quantification. \n",
        "\n",
        "Reprendre le modèle entraîné sur MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "11d35b87030b49eb99c77c4a6fe47db3",
        "deepnote_cell_type": "code",
        "id": "1rMve9WZLAjX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "af90aac8bbb94f86b728031f5ae0a66f",
        "deepnote_cell_type": "markdown",
        "id": "mHJ6TheCLAjX",
        "tags": []
      },
      "source": [
        "A l'aide de la fonction quantize de tensorflow_model_optimization, créer une seconde version de votre modèle entraîné nommé qat_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch-yHJUruzhi",
        "outputId": "a8c88be1-bf93-42d2-a46f-1076f2b6a92b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-model-optimization in /usr/local/lib/python3.9/dist-packages (0.7.3)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.9/dist-packages (from tensorflow-model-optimization) (1.16.0)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.9/dist-packages (from tensorflow-model-optimization) (1.22.4)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow-model-optimization) (0.1.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gS0SAENreTt"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "qat_model = tfmot.quantization.keras.quantize_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "868beffe9e8740d98cbfd8458af4117a",
        "deepnote_cell_type": "markdown",
        "id": "1U44IZKwLAjX",
        "tags": []
      },
      "source": [
        "Compiler le modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mD1vDbQu7s6"
      },
      "outputs": [],
      "source": [
        "qat_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4dddea973f404fb699b134a67d1a2dc5",
        "deepnote_cell_type": "markdown",
        "id": "msl2osFWLAjY",
        "tags": []
      },
      "source": [
        "Afficher le summury du modèle. D'après vous ce modèle est-il quantifié ? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL8OdT7AvH_S",
        "outputId": "24b3dd6d-c106-4b04-9ae9-b2abc6b22091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer_1 (QuantizeL  (None, 28, 28, 1)        3         \n",
            " ayer)                                                           \n",
            "                                                                 \n",
            " quant_conv2d_2 (QuantizeWra  (None, 26, 26, 32)       387       \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_conv2d_3 (QuantizeWra  (None, 24, 24, 64)       18627     \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_max_pooling2d_1 (Quan  (None, 12, 12, 64)       1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_flatten_2 (QuantizeWr  (None, 9216)             1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_dense_2 (QuantizeWrap  (None, 200)              1843405   \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dense_3 (QuantizeWrap  (None, 10)               2015      \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,864,439\n",
            "Trainable params: 1,864,226\n",
            "Non-trainable params: 213\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "qat_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7b3a873512174205b6d6f4aacd7480c8",
        "deepnote_cell_type": "markdown",
        "id": "z1Fr4U0QLAjZ",
        "tags": []
      },
      "source": [
        "Réentraîner votre modèle sur un sous ensemble des modèles sur une ou deux epochs et afficher la performance sur le train et test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8Qnb3GZvnC6",
        "outputId": "62cbbc4f-9bea-43c6-9a9b-1e2b2de65f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 266s 139ms/step - loss: 0.0385 - accuracy: 0.9879 - val_loss: 0.0338 - val_accuracy: 0.9891\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 253s 135ms/step - loss: 0.0196 - accuracy: 0.9933 - val_loss: 0.0302 - val_accuracy: 0.9911\n",
            "313/313 [==============================] - 10s 32ms/step - loss: 0.0302 - accuracy: 0.9911\n",
            "Test set accuracy: 99.11%\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0081 - accuracy: 0.9976\n",
            "Training set accuracy: 99.76%\n"
          ]
        }
      ],
      "source": [
        "qat_model.fit(x_train, y_train, epochs=2, validation_data=(x_test, y_test))\n",
        "\n",
        "loss, accuracy = qat_model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test set accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "loss, accuracy = qat_model.evaluate(x_train, y_train)\n",
        "\n",
        "print(\"Training set accuracy: {:.2f}%\".format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "cc4f49a0e7894b8385e90c2330a55bb1",
        "deepnote_cell_type": "markdown",
        "id": "pgpBWibyLAjZ",
        "tags": []
      },
      "source": [
        "Convertir votre modèle avec TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "68918464bfb14b749d3848fe25b5e02e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "ddbXpXqmLAja",
        "outputId": "8d7873cd-c8dd-49d4-ab4f-30046f5946cd",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, conv2d_2_layer_call_fn, conv2d_2_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, conv2d_3_layer_call_fn while saving (showing 5 of 13). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "qat_model_quantized_optimized = converter.convert()\n",
        "\n",
        "with open('qat_model_quantized.tflite', 'wb') as f:\n",
        "  f.write(qat_model_quantized_optimized)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertir le model en .h pour le mettre sur l'Arduino"
      ],
      "metadata": {
        "id": "vpiG4TcZc_iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat qat_model_quantized.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n"
      ],
      "metadata": {
        "id": "6vviAG8kZEqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d80e6fbfea9646919a9cbe2c8fa41b2d",
        "deepnote_cell_type": "markdown",
        "id": "ikJtF5mtLAja",
        "tags": []
      },
      "source": [
        "Comparer la performance du modèle Quantified aware training, au modèle original et au modèle quantifié post training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "N-iYhQQ2mR4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_tflite(tflite_model):\n",
        "\n",
        "  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details() \n",
        "\n",
        "  y_pred = []\n",
        "\n",
        "  for i in range(len(x_test)):\n",
        "      input_data = np.expand_dims(x_test[i], axis=0).astype(input_details[0]['dtype'])\n",
        "      interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "      interpreter.invoke()\n",
        "      output = interpreter.tensor(output_details[0]['index'])\n",
        "      y_pred.append(np.argmax(output()[0]))\n",
        "\n",
        "  # Comparaison entre la prédiction et la vraie valeur de test\n",
        "  y_pred = np.array(y_pred)\n",
        "  accuracy = (y_pred == y_test).mean()\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "UE4dCIQYh_Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "bdd43405f5fd46ce892d957bb43a353a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "id": "XEh0uTM8LAja",
        "outputId": "619d4a5f-eb5e-4527-f602-ed2207bba575",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 9s 28ms/step - loss: 0.0460 - accuracy: 0.9849\n",
            "Performance du modèle Quantified aware training : accuracy = 0.9911\n",
            "Performance du modèle original : loss = 0.0460, accuracy = 0.9849\n",
            "Performance du modèle Post training : accuracy = 0.9849\n"
          ]
        }
      ],
      "source": [
        "# Evaluation du modèle Quantified aware training\n",
        "quantized_aware_train_accuracy = evaluate_tflite(qat_model_quantized_optimized)\n",
        "\n",
        "# Evaluation du modèle original\n",
        "original_loss, original_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Evaluation du modèle Post training\n",
        "post_training_accuracy = evaluate_tflite(model_quantized_optimized)\n",
        "\n",
        "print(\"Performance du modèle Quantified aware training : accuracy = {:.4f}\".format(quantized_aware_train_accuracy))\n",
        "print(\"Performance du modèle original : loss = {:.4f}, accuracy = {:.4f}\".format(original_loss, original_accuracy))\n",
        "print(\"Performance du modèle Post training : accuracy = {:.4f}\".format(post_training_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6472c7a129394639838f76381e562588",
        "deepnote_cell_type": "markdown",
        "id": "YDVRycyrLAja",
        "tags": []
      },
      "source": [
        "Sauvegarder le modèle QAT et comparer les tailles des modèles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "d3c2951a1fd14be3b5e3c85e05a056ea",
        "deepnote_cell_type": "code",
        "id": "9k_XJ6YRLAja",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7eb7abd-9a2f-4e8a-9ea0-ef75630fb047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du modèle Quantified aware training :  1872256 octets\n",
            "Taille du modèle original :  22414000 octets\n",
            "Taille du modèle Post training: 1870216 octets\n"
          ]
        }
      ],
      "source": [
        "qat_model.save(\"qat_model.h5\")\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Taille du modèle Quantified aware training : \", os.path.getsize('qat_model_quantized.tflite'), \"octets\")\n",
        "print(\"Taille du modèle original : \", os.path.getsize('model.h5'), \"octets\")\n",
        "print(\"Taille du modèle Post training:\", os.path.getsize('model_quantized_optimized.tflite'), \"octets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4a006a6d1a194e418072192da1a920de",
        "deepnote_cell_type": "markdown",
        "id": "qWExy_CgLAjb",
        "tags": []
      },
      "source": [
        "Bonus : déployer votre modèle sur votre téléphone ou un dispositif embarqué si vous en disposez d'un. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "927af747794641fa936ad727c6bf75d0",
        "deepnote_cell_type": "code",
        "id": "JTNnIx_sLAjb",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e7635cb278ab43589a851e2c2de0d8ef",
        "deepnote_cell_type": "markdown",
        "id": "rXTwb4OYLAjb",
        "tags": []
      },
      "source": [
        "Bonus : Obtenir un modèle qui sera à la fois quantifié et élagué (prunned) en s'aidant de la documentation (https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "a1eacb94d0aa4dbd872c8babacf8bb8d",
        "deepnote_cell_type": "code",
        "id": "a0AKJmvLLAjb",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "71e07172d7514f8f84e744177e1bfb2b",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "8QQSEYgzLAjc",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "A l'aide de tensorflow lite / tensorflow lite micro "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "9UAYM_B-LAjc",
        "tags": []
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0d51e245-899d-41d6-b23b-cf3e4bbbc6ea' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "fb1d23f975ba410e92fed9f5b8cbb7e6",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}